{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69aae9f-03a3-4169-ae3e-e99f81e3e519",
   "metadata": {},
   "source": [
    "# Data Analytics and Visualization (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834dde3c-30de-4f77-ac17-871aeb695e62",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5503f3e-fe4f-47f2-8bc4-cac33bfd1c5c",
   "metadata": {},
   "source": [
    "Data Cleaning involves identifying and rectifying errors, inconsistencies, and inaccuracies within the dataset. By eliminating missing values, outliers, and redundant information, data quality is enhanced, leading to more accurate and reliable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704320f5-bcf0-43ea-b553-fdc7e55ba5a1",
   "metadata": {},
   "source": [
    "Data cleaning and preparation helps to improve the overall efficiency of data analysis processes by ensuring that the dataset is more standardized and easier to work with. It also helps to reduce the risk of making faulty decisions based on flawed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59b90d-fe4d-4f75-88d6-fa2a898a664d",
   "metadata": {},
   "source": [
    "### Handling Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71baa96a-da42-4a12-802e-10617ed4a265",
   "metadata": {},
   "source": [
    "Missing values in a dataset can hinder analysis and modeling. Pandas provides functions to handle missing values, such as  **fillna()**, which allows us to fill ***NaN*** values with a specific value or method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec77051-48c4-4e1a-aa77-7924324b805c",
   "metadata": {},
   "source": [
    "Let's first start by importing our libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7a672e8-ec35-40cc-80c8-53bc5456ca4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded1384-6e75-4ce5-85ee-fbf066212fdb",
   "metadata": {},
   "source": [
    "Now, let's practice filling ***NaN*** values using **fillna()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ea1a2e-44bb-4ef6-b819-e73e9afd4427",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame before filling missing values:\n",
      "      A   B\n",
      "0  1.0  10\n",
      "1  2.0  20\n",
      "2  NaN  30\n",
      "3  4.0  40\n",
      "4  5.0  50\n",
      "DataFrame with missing values filled:\n",
      "      A   B\n",
      "0  1.0  10\n",
      "1  2.0  20\n",
      "2  0.0  30\n",
      "3  4.0  40\n",
      "4  5.0  50\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'A': [1,2, np.nan, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"DataFrame before filling missing values:\\n\", df)\n",
    "\n",
    "df_filled = df.fillna(0)\n",
    "\n",
    "print(\"DataFrame with missing values filled:\\n\", df_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bdf045-5777-4b5e-b158-675244ff8a41",
   "metadata": {},
   "source": [
    "**[Question 2: Complete the 'handle_missing_values' function to handle missing values in the dataset]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98fe26-62ba-4291-8b12-2d1f7b2b2c7e",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d49cf-aaf4-41f7-8fc5-194ce1b48b12",
   "metadata": {},
   "source": [
    "Outliers are extreme values that can skew analysis and modeling results. Pandas can help us identify and handle outliers. In this example, we identify outliers using the **interquartile range (IQR)** method and remove them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b00bf9-125b-42bd-8863-d6ee335aacba",
   "metadata": {},
   "source": [
    "The **Interquartile Range (IQR)** is a measure of statistical dispersion, representing the range within which the middle 50% of the data lies. It is calculated as the difference between the 75th percentile (also called the third quartile, or Q3) and the 25th percentile (the first quartile, or Q1) of a dataset.\n",
    "\n",
    "The IQR is useful in identifying the spread of data, and it is commonly used to detect outliers. Values that are significantly lower than Q1 or significantly higher than Q3 are often considered outliers.\n",
    "\n",
    "Here's a breakdown of what the IQR represents in pandas and how to compute it:\n",
    "\n",
    "**What IQR Represents**\n",
    "-  Q1 (25th percentile): The value below which 25% of the data falls.\n",
    "-  Q3 (75th percentile): The value below which 75% of the data falls.\n",
    "-  IQR: The range between Q1 and Q3, calculated as IQR = Q3 - Q1.\n",
    "\n",
    "**How to Calculate IQR in pandas:**\n",
    "To calculate the IQR for a specific column in a pandas DataFrame, you can use the quantile method to get the 25th and 75th percentiles, and then subtract them to find the IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4230219-938b-48f7-a959-09df92495705",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Outliers:\n",
      "    A    B\n",
      "0  1   10\n",
      "1  2   20\n",
      "2  3   30\n",
      "3  4  200\n",
      "4  5   50\n",
      "DataFrame without outliers:\n",
      "    A   B\n",
      "0  1  10\n",
      "1  2  20\n",
      "2  3  30\n",
      "4  5  50\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'A': [1,2,3,4,5],\n",
    "    'B': [10,20,30,200,50]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame with Outliers:\\n\", df)\n",
    "\n",
    "q1 = df['B'].quantile(0.25)\n",
    "q3 = df['B'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "df_no_outliers = df[(df['B'] >= lower_bound) & (df['B'] <= upper_bound)]\n",
    "\n",
    "print(\"DataFrame without outliers:\\n\", df_no_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982fefd-3853-40be-b726-861467479fff",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "-  **q1** and **q3** are calculated using the **quantile()** function, representing the first and third quartiles of column ‘B’.\n",
    "-  **q1** represents the value below which 25% of the data lies. For column ‘B’, **q1** would be the median of the first half of the sorted values, which is 15.\n",
    "-  **q3** represents the value below which 75% of the data lies. For column ‘B’, **q3** would be the median of the second half of the sorted values, which is 50.\n",
    "-  **iqr** (Interquartile Range) is computed as the difference between **q3** and **q1**.\n",
    "-  **lower_bound** and **upper_bound** are calculated to define the thresholds beyond which data points are considered outliers. These bounds are defined as 1.5 times the IQR below q1 and above q3.\n",
    "-  The line **df_no_outliers = df[(df['B'] >= lower_bound) & (df['B'] <= upper_bound)]** filters the DataFrame to keep only the rows where the values in column ‘B’ fall within the acceptable range, effectively removing the outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c69c2-c5d7-4c11-b3cd-108e9ffd78c6",
   "metadata": {},
   "source": [
    "**[Question 3: Complete the 'handle_outliers' function to handel outliers in the dataset]]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30162c8-2de2-40fe-80e9-059c6f9825c8",
   "metadata": {},
   "source": [
    "### Dealing with Duplicate Data\n",
    "Duplicate data can lead to misleading analysis. Pandas provides functions to detect and remove duplicate rows. Here’s how we can do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a0f465-712e-461b-ab21-758ff097cf0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with duplicates:\n",
      "    A   B\n",
      "0  1  10\n",
      "1  2  20\n",
      "2  2  20\n",
      "3  3  30\n",
      "4  4  40\n",
      "5  4  40\n",
      "Duplicated Rows:\n",
      "    A   B\n",
      "2  2  20\n",
      "5  4  40\n",
      "Dataframe after dropping duplicates:\n",
      "    A   B\n",
      "0  1  10\n",
      "1  2  20\n",
      "3  3  30\n",
      "4  4  40\n"
     ]
    }
   ],
   "source": [
    " data = {\n",
    "     'A': [1,2,2,3,4,4],\n",
    "     'B': [10,20,20,30,40,40]\n",
    " }\n",
    "duplicated_df = pd.DataFrame(data)\n",
    "print(\"Dataframe with duplicates:\\n\", duplicated_df)\n",
    "\n",
    "duplicated_rows = duplicated_df[duplicated_df.duplicated()]\n",
    "\n",
    "deduplicated_df = duplicated_df.drop_duplicates(keep = 'first')\n",
    "\n",
    "print(\"Duplicated Rows:\\n\", duplicated_rows)\n",
    "print(\"Dataframe after dropping duplicates:\\n\", deduplicated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8444202-00e6-4ea2-a908-900498c37756",
   "metadata": {},
   "source": [
    "**[Question 4: Complete the 'handle_duplicates' function to handle outliers in the dataset]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79085465-c240-43e9-abca-826c67c31a7f",
   "metadata": {},
   "source": [
    "### Data Reshaping\n",
    "Reshaping data is the process of transforming data from one format to another. In the context of data analysis and machine learning (ML), reshaping data often involves reorganizing it into a different structure that is better suited for analysis, visualization, or modeling. Reshaping can involve tasks such as pivoting, melting, stacking, unstacking, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d4205-2b6d-43ed-bdf8-ae31f69d461d",
   "metadata": {},
   "source": [
    "#### Wide to Long Format (Melting)\n",
    "In this transformation, we convert a dataset from a wide format (many columns) to a long format (fewer columns) by melting or unpivoting it. This is useful when we have variables stored as columns and we want to gather them into a single column.\n",
    "\n",
    "Melting data is useful for making it more suitable for analysis, especially when we want to compare or aggregate across different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb393e16-ed56-45a1-ac56-ad814798f288",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Wide DataFrame:\n",
      "    Depth  Temperature  Salinity\n",
      "0     10         15.0      33.0\n",
      "1     50         12.0      34.5\n",
      "2    100          8.5      36.0\n",
      "\n",
      "Long Format DataFrame:\n",
      "    Depth     variable  value\n",
      "0     10  Temperature   15.0\n",
      "1     50  Temperature   12.0\n",
      "2    100  Temperature    8.5\n",
      "3     10     Salinity   33.0\n",
      "4     50     Salinity   34.5\n",
      "5    100     Salinity   36.0\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Depth' : [10, 50, 100],\n",
    "    'Temperature': [15.0, 12.0, 8.5],\n",
    "    'Salinity': [33.0, 34.5, 36.0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original Wide DataFrame:\\n\", df)\n",
    "\n",
    "df_long = pd.melt(df, id_vars = ['Depth'],\n",
    "                  value_vars = ['Temperature', 'Salinity'],\n",
    "                  var_name= 'Measurement', value_name = 'Value')\n",
    "print(\"\\nLong Format DataFrame:\\n\", df_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf7b0e-276c-4462-9436-2bc545313fc5",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "-  The **pd.melt()** function is used to transform the *df* DataFrame from wide format to long format.\n",
    "-  **id_vars=['Depth (m)']** keeps the ‘Depth (m)’ column as an identifier. This means that each temperature and salinity reading will be linked to its depth.\n",
    "\n",
    "- **value_vars=['Temperature (°C)', 'Salinity (ppt)']** tells the function which columns to transform. In this case, we are melting the ‘Temperature (°C)’ and ‘Salinity (ppt)’ columns into a single column. This helps us look at temperature and salinity together.\n",
    "\n",
    "- **var_name='Measurement'** sets the name for the new column that will show what type of measurement each value represents (either temperature or salinity).\n",
    "\n",
    "- **value_name='Value'** sets the name for the new column that will show the actual measurement values for temperature and salinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e66fd-1dc1-4e9e-b878-c93cbfa71810",
   "metadata": {},
   "source": [
    "#### Long to Wide Format (Pivoting)\n",
    "This transformation involves converting a long-format dataset back into a wide format by pivoting or spreading the values.\n",
    "\n",
    "Pivoting is useful when we want to reshape data to make it easier to visualize or perform calculations on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dda1eb1-017d-49ad-bd69-27e5437c5815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Long DataFrame:\n",
      "    Depth  Measurement  Value\n",
      "0     10  Temperature   15.0\n",
      "1     10     Salinity   33.0\n",
      "2     50  Temperature   12.0\n",
      "3     50     Salinity   34.5\n",
      "Wide Format DataFrame:\n",
      " Measurement  Salinity  Temperature\n",
      "Depth                             \n",
      "10               33.0         15.0\n",
      "50               34.5         12.0\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Depth': [10,10,50,50],\n",
    "    'Measurement': ['Temperature', 'Salinity', 'Temperature', 'Salinity'],\n",
    "    'Value': [15.0, 33.0, 12.0, 34.5]\n",
    "}\n",
    "\n",
    "df_long = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original Long DataFrame:\\n\", df_long)\n",
    "\n",
    "df_wide = df_long.pivot(index='Depth', columns = 'Measurement',\n",
    "                        values = 'Value')\n",
    "print(\"Wide Format DataFrame:\\n\", df_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17881c55-4323-4c3a-b45f-357ef2dc60b2",
   "metadata": {},
   "source": [
    "**Code Explanation** \n",
    "-  The **df_long.pivot()** function is used to transform the df_long DataFrame from a long format to a wide format.\n",
    "-  **index='Depth (m)'** specifies that the new DataFrame should use the 'Depth (m)' column as the row labels. This means each unique depth will become a row in the new DataFrame.\n",
    "- **columns='Measurement'** tells the function to use the values in the 'Measurement' column (Temperature (°C) and Salinity (ppt)) as the column headers in the new DataFrame.\n",
    "- **values='Value'** indicates that the actual data in the new DataFrame will come from the 'Value' column. So, it will fill the cells with the corresponding temperature and salinity values at each depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13195248-d6b9-49b8-b2a4-42447a84dffa",
   "metadata": {},
   "source": [
    "### Stacking and Unstacking\n",
    "Stacking involves converting columns into rows, and unstacking is the reverse process. These operations can be useful for creating hierarchical indexes and dealing with multi-level data.\n",
    "\n",
    "Stacking and unstacking can make data manipulation and analysis easier when dealing with multi-indexed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fce57447-4ef1-44bf-b6c6-1a2f3d750303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    Depth  Temperature  Salinity\n",
      "0     10         15.0      33.0\n",
      "1     50         12.0      34.5\n",
      "Stacked DataFrame:\n",
      " Depth             \n",
      "10     Temperature    15.0\n",
      "       Salinity       33.0\n",
      "50     Temperature    12.0\n",
      "       Salinity       34.5\n",
      "dtype: float64\n",
      "Unstacked DataFrame:\n",
      "        Temperature  Salinity\n",
      "Depth                       \n",
      "10            15.0      33.0\n",
      "50            12.0      34.5\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Depth': [10,50],\n",
    "    'Temperature': [15.0, 12.0],\n",
    "    'Salinity': [33.0, 34.5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "df.set_index('Depth', inplace = True)\n",
    "\n",
    "stacked_df = df.stack()\n",
    "unstacked_df = stacked_df.unstack()\n",
    "print(\"Stacked DataFrame:\\n\", stacked_df)\n",
    "print(\"Unstacked DataFrame:\\n\", unstacked_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962a1aa-e01e-4302-aef5-13e4c7449153",
   "metadata": {},
   "source": [
    "Stacking transforms a DataFrame from a wide format to a long format by moving a level of the column index to the row index. This operation is useful when you want to collapse hierarchical column indices into a simpler format.\n",
    "\n",
    "Unstacking is the reverse of stacking. It transforms a DataFrame from a long format to a wide format by moving a level of the row index to the column index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c3fb5-b06a-4a59-8b3f-8bf9fd6fb488",
   "metadata": {},
   "source": [
    "### Handling Inconsistent Data and Standardizing\n",
    "Handling inconsistent data is a crucial step in data preprocessing to ensure the accuracy and reliability of our analysis or modeling. Inconsistent data refers to values that do not adhere to the expected format or constraints. This can include typos, varying representations, or unexpected values in categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23a7fe-0621-4aec-a2b8-c8453a8a6ad0",
   "metadata": {},
   "source": [
    "Suppose we have a dataset with a “Water_Type” column that contains variations of the categories “Freshwater”, “Salty”, and “Brackish”. To handle inconsistencies, we can standardize the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a815113-5787-4e5c-87c9-3e5efd09a146",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    Sample_ID   Water_Type\n",
      "0          1   Freshwater\n",
      "1          2        salty\n",
      "2          3   FRESHWATER\n",
      "3          4        Salty\n",
      "4          5     brackish\n",
      "5          6    BRACKISH \n",
      "DataFrame with Consistent Water type values:\n",
      "    Sample_ID  Water_Type\n",
      "0          1  Freshwater\n",
      "1          2       Salty\n",
      "2          3  Freshwater\n",
      "3          4       Salty\n",
      "4          5    Brackish\n",
      "5          6    Brackish\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Sample_ID': [1,2,3,4,5,6],\n",
    "    'Water_Type': ['Freshwater', 'salty', ' FRESHWATER',\n",
    "                   'Salty', 'brackish','BRACKISH ']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "df['Water_Type'] = df['Water_Type'].str.lower().str.strip().replace(\n",
    "        {'freshwater':'Freshwater', 'salty': 'Salty', 'brackish': 'Brackish'}\n",
    ")\n",
    "\n",
    "print(\"DataFrame with Consistent Water type values:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c80b98-9249-4cde-8e3b-cb982e6d1cc4",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "- **df['Water_Type']:** This gets the 'Water_Type' column from the DataFrame so we can work with it.\n",
    "\n",
    "- **.str.lower()**: This changes all the text in the 'Water_Type' column to lowercase. This way, we make sure everything is written the same way.\n",
    "\n",
    "- **.str.strip()**: This removes any extra spaces at the start or end of the text in the 'Water_Type' column. This helps avoid mistakes caused by those spaces.\n",
    "\n",
    "- **.replace({'freshwater': 'Freshwater', 'salty': 'Salty', 'brackish': 'Brackish'})**: This replaces the words in the 'Water_Type' column. It changes 'freshwater' to 'Freshwater', 'salty' to 'Salty', and 'brackish' to 'Brackish'. This makes sure we have a consistent way of writing these terms.\n",
    "\n",
    "- **Updating the Column**: Finally, we save the cleaned-up values back into the 'Water_Type' column. This makes the data neat and ready for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54861d-1de0-4dfa-b4ff-dab900976899",
   "metadata": {},
   "source": [
    "Now, suppose we have a dataset with a “Water_Type” column that contains various names for types of water, including some inconsistent spellings and synonyms. We want to standardize these water type names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f0b697b-7466-4a08-ba7b-bc97249283b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    ID  Water_Type\n",
      "0   1  freshwater\n",
      "1   2       salty\n",
      "2   3    brackish\n",
      "3   4       Salty\n",
      "4   5  FRESHWATER\n",
      "Dataframe with consistent water type names:\n",
      "    ID  Water_Type\n",
      "0   1  Freshwater\n",
      "1   2       Salty\n",
      "2   3    Brackish\n",
      "3   4       Salty\n",
      "4   5  Freshwater\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'ID':[1,2,3,4,5],\n",
    "    'Water_Type': ['freshwater', 'salty', 'brackish','Salty', 'FRESHWATER']\n",
    "}\n",
    "df= pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\\n\", df)\n",
    "\n",
    "water_type_mapping = {\n",
    "    'freshwater': 'Freshwater',\n",
    "    'salty': 'Salty',\n",
    "    'brackish': 'Brackish'\n",
    "}\n",
    "\n",
    "df['Water_Type'] = df['Water_Type'].str.lower().str.strip().map(water_type_mapping)\n",
    "\n",
    "print(\"Dataframe with consistent water type names:\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99cc3c-4f56-4a8e-84a0-f340635bbc7a",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "- **Mapping Definition:** A dictionary called water_type_mapping is created to map the inconsistent names to standardized names. For example, both 'freshwater' and 'FRESHWATER' will be converted to 'Freshwater'.\n",
    "\n",
    "- **Applying the Mapping:** We convert the ‘Water_Type’ column to lowercase and use .map(water_type_mapping) to replace the values based on our defined mapping.\n",
    "\n",
    "- **Final Output:** Finally, we print the updated DataFrame, which now contains consistent water type names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeab8a8-ae8d-43f5-8387-ed93231bc7e4",
   "metadata": {},
   "source": [
    " **[Question 5: Complete the 'standardize_data' function to standardizes the 'Species' column in the dataset]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
